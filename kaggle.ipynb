{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK:\n",
    "Improve test score to 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/whats-cooking-kernels-only/data\n",
    "# https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_df = pd.read_json('all/train.json')\n",
    "# train_df = train_df[(train_df['cuisine'] == 'russian') | (train_df['cuisine'] == 'brazilian')]\n",
    "test_df = pd.read_json('all/test.json')\n",
    "# train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index([u'cuisine', u'id', u'ingredients'], dtype='object'),\n",
       " Index([u'id', u'ingredients'], dtype='object'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns, test_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuisine        0\n",
      "id             0\n",
      "ingredients    0\n",
      "dtype: int64\n",
      "id             0\n",
      "ingredients    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Detect  missing values\n",
    "print(train_df.isna().sum())\n",
    "print(test_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuisine</th>\n",
       "      <th>id</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>greek</td>\n",
       "      <td>10259</td>\n",
       "      <td>[romaine lettuce, black olives, grape tomatoes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>southern_us</td>\n",
       "      <td>25693</td>\n",
       "      <td>[plain flour, ground pepper, salt, tomatoes, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>filipino</td>\n",
       "      <td>20130</td>\n",
       "      <td>[eggs, pepper, salt, mayonaise, cooking oil, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>indian</td>\n",
       "      <td>22213</td>\n",
       "      <td>[water, vegetable oil, wheat, salt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>indian</td>\n",
       "      <td>13162</td>\n",
       "      <td>[black pepper, shallots, cornflour, cayenne pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cuisine     id                                        ingredients\n",
       "0        greek  10259  [romaine lettuce, black olives, grape tomatoes...\n",
       "1  southern_us  25693  [plain flour, ground pepper, salt, tomatoes, g...\n",
       "2     filipino  20130  [eggs, pepper, salt, mayonaise, cooking oil, g...\n",
       "3       indian  22213                [water, vegetable oil, wheat, salt]\n",
       "4       indian  13162  [black pepper, shallots, cornflour, cayenne pe..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible issues:\n",
    "# multiple ID\n",
    "# spelling mistakes\n",
    "# spelling case\n",
    "# ingredients multiple times\n",
    "# singular / plural\n",
    "# different names for same ingredient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "italian         7838\n",
       "mexican         6438\n",
       "southern_us     4320\n",
       "indian          3003\n",
       "chinese         2673\n",
       "french          2646\n",
       "cajun_creole    1546\n",
       "thai            1539\n",
       "japanese        1423\n",
       "greek           1175\n",
       "spanish          989\n",
       "korean           830\n",
       "vietnamese       825\n",
       "moroccan         821\n",
       "british          804\n",
       "filipino         755\n",
       "irish            667\n",
       "jamaican         526\n",
       "russian          489\n",
       "brazilian        467\n",
       "Name: cuisine, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['cuisine'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# ML pipeline consisting of two steps\n",
    "pipeline = Pipeline(steps=[\n",
    "    \n",
    "    ('tdidf', TfidfVectorizer()),\n",
    "    ('SVC', SVC(C=100,\n",
    "                kernel='rbf',\n",
    "                gamma= 1, \n",
    "#                 probability=True, \n",
    "                class_weight=None\n",
    "               )),\n",
    "#     ('RFC', RandomForestClassifier(n_estimators= 100)),\n",
    "#     ('LR', LogisticRegression()),\n",
    "#     ('GB', GradientBoostingClassifier()),\n",
    "#     ('GBC', KNeighborsClassifier(n_neighbors=100)),\n",
    "])\n",
    "pipeline = OneVsRestClassifier(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "romaine_lettuce black_olives grape\n"
     ]
    }
   ],
   "source": [
    "list_ing = ['romaine lettuce', 'black olives', 'grape']\n",
    "\n",
    "def func(l):\n",
    "    string = \"/\".join(l)\n",
    "    result = string.replace(\" \",\"_\")\n",
    "    result2 = result.replace(\"/\",\" \")    \n",
    "    return result2\n",
    "print func(list_ing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_cols = ['ingredients']\n",
    "y_cols = ['cuisine']\n",
    "\n",
    "x = train_df.filter(items=x_cols)['ingredients'].apply(func)# turn list of strings into one string\n",
    "y = train_df.filter(items=y_cols).values.ravel()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7, test_size=0.3)\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SCORE: \n",
      "0.99974857224956\n",
      "TEST SCORE: \n",
      "0.7996312746166094\n",
      "CPU times: user 27min 58s, sys: 10.7 s, total: 28min 9s\n",
      "Wall time: 28min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipeline.fit(x_train, y_train)\n",
    "print (\"TRAIN SCORE: \")\n",
    "print(pipeline.score(x_train, y_train))\n",
    "print (\"TEST SCORE: \")\n",
    "print(pipeline.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_validate(pipeline, x, y, cv=3, return_train_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import learning_curve\n",
    "# for train_sizes, train_scores, test_scores in learning_curve(pipeline, x, y, cv=5):\n",
    "#     print (train_sizes, train_scores, test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'estimator': Pipeline(memory=None,\n",
       "      steps=[('tdidf', TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "         dtype=<type 'numpy.float64'>, encoding=u'utf-8', input=u'content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=T...,\n",
       "   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "   tol=0.001, verbose=False))]),\n",
       " 'estimator__SVC': SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf',\n",
       "   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "   tol=0.001, verbose=False),\n",
       " 'estimator__SVC__C': 100,\n",
       " 'estimator__SVC__cache_size': 200,\n",
       " 'estimator__SVC__class_weight': None,\n",
       " 'estimator__SVC__coef0': 0.0,\n",
       " 'estimator__SVC__decision_function_shape': 'ovr',\n",
       " 'estimator__SVC__degree': 3,\n",
       " 'estimator__SVC__gamma': 1,\n",
       " 'estimator__SVC__kernel': 'rbf',\n",
       " 'estimator__SVC__max_iter': -1,\n",
       " 'estimator__SVC__probability': False,\n",
       " 'estimator__SVC__random_state': None,\n",
       " 'estimator__SVC__shrinking': True,\n",
       " 'estimator__SVC__tol': 0.001,\n",
       " 'estimator__SVC__verbose': False,\n",
       " 'estimator__memory': None,\n",
       " 'estimator__steps': [('tdidf',\n",
       "   TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "           dtype=<type 'numpy.float64'>, encoding=u'utf-8', input=u'content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
       "           stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "           token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "           vocabulary=None)),\n",
       "  ('SVC', SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "     decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf',\n",
       "     max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "     tol=0.001, verbose=False))],\n",
       " 'estimator__tdidf': TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "         dtype=<type 'numpy.float64'>, encoding=u'utf-8', input=u'content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       " 'estimator__tdidf__analyzer': u'word',\n",
       " 'estimator__tdidf__binary': False,\n",
       " 'estimator__tdidf__decode_error': u'strict',\n",
       " 'estimator__tdidf__dtype': numpy.float64,\n",
       " 'estimator__tdidf__encoding': u'utf-8',\n",
       " 'estimator__tdidf__input': u'content',\n",
       " 'estimator__tdidf__lowercase': True,\n",
       " 'estimator__tdidf__max_df': 1.0,\n",
       " 'estimator__tdidf__max_features': None,\n",
       " 'estimator__tdidf__min_df': 1,\n",
       " 'estimator__tdidf__ngram_range': (1, 1),\n",
       " 'estimator__tdidf__norm': u'l2',\n",
       " 'estimator__tdidf__preprocessor': None,\n",
       " 'estimator__tdidf__smooth_idf': True,\n",
       " 'estimator__tdidf__stop_words': None,\n",
       " 'estimator__tdidf__strip_accents': None,\n",
       " 'estimator__tdidf__sublinear_tf': False,\n",
       " 'estimator__tdidf__token_pattern': u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'estimator__tdidf__tokenizer': None,\n",
       " 'estimator__tdidf__use_idf': True,\n",
       " 'estimator__tdidf__vocabulary': None,\n",
       " 'n_jobs': None}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "# Function:\n",
    "def vc(estimator, x, y, param_name, param_range, cv, p_type):\n",
    "    # calculate train and test scores\n",
    "    train_scores, test_scores = validation_curve(estimator=estimator,\n",
    "                                                X=x, \n",
    "                                                y=y, \n",
    "                                                param_name=param_name,\n",
    "                                                param_range=param_range,\n",
    "                                                cv=cv)\n",
    "    # specify xticklabels by using range when param_range is a list of strings\n",
    "    p_range = range(len(param_range))\n",
    "    # switch xtick labels with strings\n",
    "    if type(param_range[0]) == str:\n",
    "        plt.xticks(range(len(param_range)), param_range)\n",
    "        param_range = p_range\n",
    "        \n",
    "    mean_train = np.mean(train_scores, axis=1)\n",
    "    mean_test = np.mean(test_scores, axis=1)\n",
    "    plt.title(\"Validation Curve with \" + param_name.split(\"__\")[-2]+ \" on parameter \"+ param_name.split(\"__\")[-1])\n",
    "    plt.xlabel(param_name.split(\"__\")[-1])\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.ylim(0.0, 1.1)\n",
    "    # plot the train and test scores\n",
    "    if p_type == 'scatter':\n",
    "        plt.scatter(param_range, mean_train, label = \"Training score\",color=\"darkorange\", lw=2)\n",
    "        plt.scatter(param_range, mean_test, label = \"Cross-validation score\",color=\"navy\", lw=2)\n",
    "    elif p_type == 'plot':\n",
    "        plt.plot(param_range, mean_train,label = \"Training score\",color=\"darkorange\", lw=2)\n",
    "        plt.plot(param_range, mean_test, label = \"Cross-validation score\",color=\"navy\", lw=2)\n",
    "    elif p_type == 'semilogx': \n",
    "        plt.semilogx(param_range, train_scores,label = \"Training score\",color=\"darkorange\", lw=2)\n",
    "        plt.semilogx(param_range, test_scores, label = \"Cross-validation score\",color=\"navy\", lw=2)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel\n",
    "\n",
    "vc(estimator = pipeline,\n",
    "   x = x, \n",
    "   y= y, \n",
    "   param_name = \"estimator__SVC__kernel\",\n",
    "   param_range = ['rbf', 'poly', 'linear','sigmoid'], \n",
    "   cv = 3,\n",
    "   p_type = 'scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc(estimator = pipeline,\n",
    "   x = x, \n",
    "   y= y, \n",
    "   param_name = \"estimator__SVC__probability\",\n",
    "   param_range = [0, 0.5, 1], \n",
    "   cv = 3,\n",
    "  p_type = 'scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C \n",
    "vc(estimator = pipeline,\n",
    "   x = x, \n",
    "   y= y, \n",
    "   param_name = \"estimator__SVC__C\",\n",
    "   param_range = [0.1, 1, 10, 100, 1000], \n",
    "   cv = 3,\n",
    "   p_type = 'semilogx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gamma \n",
    "vc(estimator = pipeline,\n",
    "   x = x, \n",
    "   y= y, \n",
    "   param_name = \"estimator__SVC__gamma\",\n",
    "   param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0], \n",
    "   cv = 3,\n",
    "   p_type = 'semilogx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use function on other parameters\n",
    "# clean up code\n",
    "# run whole dataset \n",
    "## (bonus) make subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
